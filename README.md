# ğŸ“˜ CampusWise

**CampusWise** is a chatbot-based Student Manual system for undergraduate students of Leyte Normal University. It uses Retrieval-Augmented Generation (RAG) with Ollamaâ€™s LLMs and LangChain to answer questions from the handbook via a web interface.

---

## ğŸš€ Features

- Retrieves relevant handbook content using document embeddings
- Answers user queries through **llama3.2** LLM
- Frontend chat interface served via **Nginx**
- Backend API built with **Express.js** using RAG chain
- Easy deployment via **Docker** and **docker-compose**

---

## ğŸ§© Project Structure

```
/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ chatbot/
â”‚       â”œâ”€â”€ embeddings.js      â† embedding loader (OllamaEmbeddings + MemoryVectorStore)
â”‚       â”œâ”€â”€ llms.js            â† ChatOllama model wrapper
â”‚       â”œâ”€â”€ loader.js          â† document splitter loader
â”‚       â””â”€â”€ server.js          â† Express server with /chat endpoint
â”œâ”€â”€ resources/                 â† images, icons
â””â”€â”€ web/
    â”œâ”€â”€ html/                  â† chatbot.html, plus intro/login/register
    â”œâ”€â”€ css/                   â† chatbot.css and others
    â””â”€â”€ js/                    â† chatbot.js (frontend logic)

```

---

## ğŸ›  Pre-requisites

1. Docker & Docker Compose
2. Ollama installed and models pulled:
    
    ```bash
    ollama pull llama3.2
    ollama pull mxbai-embed-large
    ```
    
3. (Optional) Local Node.js v22.x+ for development/test

---

## ğŸ³ Running With Docker Compose

In your project root, run:

```bash
docker-compose up --build
```

- Backend: [**http://localhost:3000**](http://localhost:3000/)
- Frontend: [**http://localhost:8080**](http://localhost:8080/)

This sets up three containers:

- **backend**: runs your Express server using Node.js
- **frontend**: serves static HTML/CSS/JS via Nginx
- **ollama**: hosts LLM API & serves pulled models

---

## ğŸ”§ Development Setup (Without Docker)

1. Install Node.js 22.x+
    
    ```bash
    npm install
    ```
    
2. Run Ollama and pull models:
    
    ```bash
    ollama pull llama3.2
    ollama pull mxbai-embed-large
    ollama run --listen
    ```
    
3. Launch the backend server:
    
    ```bash
    node app/chatbot/server.js
    ```
    
4. Serve frontend files:
    
    Use VSCode Live Server or serve via Express
    
5. Visit:
    
    ```
    http://localhost:3000 => backend API
    http://localhost:5500/web/html/chatbot.html => frontend chat
    ```
    

---

## ğŸ’¬ How It Works

1. **User** types a question â†’ frontend sends it to `/chat` endpoint
2. **Server** performs similarity search on embeddings, retrieves top chunks
3. **RAG chain** passes query and context to LLM
4. **Server response** (JSON) with AIâ€™s answer
5. **Frontend** appends the AIâ€™s reply in chat UI

---

## âœ… Notes

- Ollama models are **mounted** in Docker via volumeâ€”no need to pull inside container
- The frontend JS (`web/js/chatbot.js`) handles user input, POSTs to `/chat`, and renders messages
- You can customize backend port or frontend routes in `docker-compose.yml`
- For final defense, highlight:
    - The RAG retrieval pipeline
    - Dockerized deployment for easy demonstration

---

## ğŸ§‘â€ğŸ« Credits

- **benny-18** â€“ Development & design
- **hiyaranari** â€“ Development & design
- **Leyte Normal University** â€“ Handbook content
